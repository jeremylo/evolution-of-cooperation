{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from random import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from society.action import Action\n",
    "from society.agent import Agent, TrainableAgent\n",
    "from society.simulations.weighted import WeightedNetworkSimulation\n",
    "from society.strategies.gameplay.constant import AllC, AllD\n",
    "from society.strategies.gameplay.qlearning import TabularQLearningGameplayStrategy\n",
    "from society.strategies.gameplay.random import RandomGameplayStrategy\n",
    "from society.strategies.gameplay.tft import TitForTat\n",
    "from society.visualisation.network import *\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_population(communities, size):\n",
    "    population = communities * size\n",
    "\n",
    "    agents = [\n",
    "        Agent(TabularQLearningGameplayStrategy(lookback=3, epsilon=0.1, learning_rate=0.1), i, population)\n",
    "        for i in range(population)\n",
    "    ]\n",
    "\n",
    "    G = nx.connected_caveman_graph(communities, size)\n",
    "\n",
    "    weights_matrix = np.zeros((population, population))\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        weights_matrix[u, v] = weights_matrix[v, u] = 1.0\n",
    "        try:\n",
    "            d[\"weight\"] = weights_matrix[u, v]\n",
    "        except:\n",
    "            d[\"weight\"] = 0\n",
    "\n",
    "    return agents, weights_matrix, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policies(agents):\n",
    "    policies = [\n",
    "        agent.gameplay_strategy._q_table.argmax(axis=-1)\n",
    "        for agent in agents\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        \"\".join(\n",
    "            [\n",
    "                (\"C\", \"D\")[policy[i, j, k]]\n",
    "                for i in range(4)\n",
    "                for j in range(4)\n",
    "                for k in range(4)\n",
    "            ]\n",
    "        )\n",
    "        for policy in policies\n",
    "    ]\n",
    "\n",
    "def calculate_cooperativeness(history):\n",
    "    count = history.count(Action.COOPERATE)\n",
    "\n",
    "    return count / len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAYOFF_LABELS = [\"(C, C)\", \"(C, D)\", \"(D, C)\", \"(D, D)\"]\n",
    "# PAYOFF_LABELS = [\"R\", \"S\", \"T\", \"P\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde6bdabad4c49aaaf6a783d4b84280a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Run 1:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST AGENT: 151531 (0.8748782185107864, CCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCDCCCCCCCCDCCCCCCCCCCCCCCCCCC)\n",
      "WORST AGENT: 143615 (0.937342061802336, DCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)\n"
     ]
    }
   ],
   "source": [
    "POPULATION = (1, 16)\n",
    "ROUNDS = 25_000\n",
    "\n",
    "run_rewards = []\n",
    "run_cumulative_rewards = []\n",
    "run_action_histories = []\n",
    "run_reward_histories = []\n",
    "run_policies = []\n",
    "\n",
    "for run in range(1):\n",
    "    # Generate a new population\n",
    "    agents, weights_matrix, G = generate_population(*POPULATION)\n",
    "\n",
    "    # Run a number of rounds\n",
    "    sim = WeightedNetworkSimulation(agents, weights_matrix)\n",
    "\n",
    "    # Run the simulation\n",
    "    sim.reset()\n",
    "    for i in tqdm(range(ROUNDS), desc=f\"Run {run + 1}\"):\n",
    "        sim.play_round(train=True)\n",
    "\n",
    "    # Store results\n",
    "    run_rewards.append(sim.rewards)\n",
    "    run_action_histories.append(sim.action_histories)\n",
    "    run_reward_histories.append(sim.reward_histories)\n",
    "    run_policies.append(compute_policies(agents))\n",
    "\n",
    "    # Output statistics for the best and worst agents\n",
    "    cumulative_rewards = [(i, sum(history)) for i, history in enumerate(sim.reward_histories)]\n",
    "    cumulative_rewards.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # TODO: maybe I should switch to mean rewards (?)\n",
    "    # mean_rewards = [(i, np.mean(history)) for i, history in enumerate(sim.reward_histories)]\n",
    "    # mean_rewards.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"BEST AGENT: {cumulative_rewards[0][1]} ({calculate_cooperativeness(list(chain(*sim.action_histories[cumulative_rewards[0][0]])))}, {run_policies[-1][cumulative_rewards[0][0]]})\"\n",
    "    )\n",
    "    tqdm.write(\n",
    "        f\"WORST AGENT: {cumulative_rewards[-1][1]} ({calculate_cooperativeness(list(chain(*sim.action_histories[cumulative_rewards[-1][0]])))}, {run_policies[-1][cumulative_rewards[-1][0]]})\"\n",
    "    )\n",
    "\n",
    "    # for rh in sim.reward_histories:\n",
    "    #     plt.plot(np.cumsum(rh))\n",
    "    \n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[258.85244791, 256.07213307],\n",
       "         [263.14667497, 176.88293418],\n",
       "         [267.55473068, 255.29884888],\n",
       "         [249.42754356,  37.11568336]],\n",
       "\n",
       "        [[254.43528773, 269.29796889],\n",
       "         [248.06996277,  61.00552607],\n",
       "         [238.01940145,  11.24905851],\n",
       "         [ 65.01964217,   0.        ]],\n",
       "\n",
       "        [[267.50826318, 265.91731419],\n",
       "         [265.85541276,  77.60073739],\n",
       "         [262.28001163,  92.38964146],\n",
       "         [126.80438918,   0.5       ]],\n",
       "\n",
       "        [[257.17937024,  98.03574845],\n",
       "         [209.38802128,  10.19324086],\n",
       "         [ 89.04572376,   3.49449315],\n",
       "         [ 17.01751472,   0.        ]]],\n",
       "\n",
       "\n",
       "       [[[258.36292192, 211.21590507],\n",
       "         [212.04746573,   0.        ],\n",
       "         [267.92187826, 246.38918269],\n",
       "         [239.53263243,   8.0254341 ]],\n",
       "\n",
       "        [[256.78407515, 172.78224022],\n",
       "         [154.2284775 ,   6.27062235],\n",
       "         [152.45109014,   0.        ],\n",
       "         [  8.17536402,   0.        ]],\n",
       "\n",
       "        [[263.79925288, 112.84056938],\n",
       "         [176.75064028,   3.80691721],\n",
       "         [ 78.43815009,  17.34518115],\n",
       "         [  0.        ,   0.        ]],\n",
       "\n",
       "        [[153.42636744,  29.405476  ],\n",
       "         [  0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ]]],\n",
       "\n",
       "\n",
       "       [[[258.30002215, 268.83918981],\n",
       "         [268.20490518, 202.88431839],\n",
       "         [266.87908084, 148.79041298],\n",
       "         [214.89482139,   0.60899735]],\n",
       "\n",
       "        [[205.12630658, 267.81198223],\n",
       "         [191.8373625 ,   0.7169733 ],\n",
       "         [165.841768  ,  12.15822589],\n",
       "         [ 11.37557647,   0.        ]],\n",
       "\n",
       "        [[267.3199817 , 156.24803935],\n",
       "         [199.18869444,  13.51447719],\n",
       "         [180.89237761,   0.        ],\n",
       "         [ 21.84044709,   0.        ]],\n",
       "\n",
       "        [[212.44301184,   4.37512367],\n",
       "         [  0.        ,   4.63044692],\n",
       "         [ 17.49226166,   0.        ],\n",
       "         [  0.        ,   0.        ]]],\n",
       "\n",
       "\n",
       "       [[[257.99706475, 203.39924849],\n",
       "         [192.81084715,  22.99778628],\n",
       "         [201.09456565,  31.09854222],\n",
       "         [ 18.08541961,   0.        ]],\n",
       "\n",
       "        [[247.04398426,  19.16423073],\n",
       "         [ 86.32114738,   0.        ],\n",
       "         [ 81.35415021,   0.        ],\n",
       "         [  0.        ,   0.        ]],\n",
       "\n",
       "        [[191.51159457,  25.43957731],\n",
       "         [ 63.048763  ,   0.        ],\n",
       "         [ 49.03074376,   0.        ],\n",
       "         [  7.59171613,   0.        ]],\n",
       "\n",
       "        [[ 80.37926698,  13.73041242],\n",
       "         [  0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[cumulative_rewards[0][0]].gameplay_strategy._q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "with open(f\"results - {datetime.isoformat(datetime.now()).replace(':', '-')}.pickle\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"rewards\": run_rewards,\n",
    "        \"cumulative_rewards\": run_cumulative_rewards,\n",
    "        # \"action_histories\": run_action_histories,\n",
    "        \"reward_histories\": run_reward_histories,\n",
    "        \"policies\": run_policies,\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_reward_policy_ranks = {}\n",
    "cumulative_reward_policies = {}\n",
    "\n",
    "for policies, reward_histories in zip(run_policies, run_reward_histories):\n",
    "    cumulative_rewards = [(i, sum(history)) for i, history in enumerate(reward_histories)]\n",
    "    cumulative_rewards.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for rank, (i, cumulative_reward) in enumerate(cumulative_rewards):\n",
    "        policy = policies[i]\n",
    "\n",
    "        if policy not in cumulative_reward_policy_ranks:\n",
    "            cumulative_reward_policy_ranks[policy] = []\n",
    "            cumulative_reward_policies[policy] = []\n",
    "\n",
    "        cumulative_reward_policy_ranks[policy].append(rank + 1)\n",
    "        cumulative_reward_policies[policy].append(cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward_policy_ranks = {policy: np.mean(ranks) for policy, ranks in cumulative_reward_policy_ranks.items()}\n",
    "mean_cumulative_reward_policies = {policy: np.mean(rewards) for policy, rewards in cumulative_reward_policies.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0                      CCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCDCCCCCCCCDCCCCCCCCCCCCCCCCCC\n",
      "2.0                      DDDCCDCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCDCCCCDCCCCCCCCC\n",
      "3.0                      DCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCDCCCCCCCCCCCCCCDCCCCCCCCCCCCC\n",
      "4.0                      DCCCCCCCCDCCCCCCCCCCCCCDCCCCDCCCDCCDCCCCCDCCCCCCCCCCCCCCCCCCCCCC\n",
      "5.0                      CCCCDCCCCDCDCCCCCCCCCCCCCCCCCDCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCDCCC\n",
      "6.0                      DCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "7.0                      CCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCC\n",
      "8.0                      DCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "9.0                      CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "10.0                     DCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCDCDCCCCCDCCCDCCCCCCC\n",
      "11.0                     DCCCCCCCCCCCCCCCDCCCCCCCCCDCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "12.0                     DCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "13.0                     CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCDCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "14.0                     CCCCDCCCCCCCCDCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "15.0                     DCCCCCCCCCCCCCDCDCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "16.0                     DCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "for policy in sorted(mean_cumulative_reward_policy_ranks, key=lambda x: mean_cumulative_reward_policy_ranks[x]):\n",
    "    print(f\"{mean_cumulative_reward_policy_ranks[policy]:<24} {policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151531.0                 CCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCDCCCCCCCCDCCCCCCCCCCCCCCCCCC\n",
      "150599.0                 DDDCCDCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCDCCCCDCCCCCCCCC\n",
      "146640.0                 DCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCDCCCCCCCCCCCCCCDCCCCCCCCCCCCC\n",
      "145859.0                 DCCCCCCCCDCCCCCCCCCCCCCDCCCCDCCCDCCDCCCCCDCCCCCCCCCCCCCCCCCCCCCC\n",
      "145435.0                 CCCCDCCCCDCDCCCCCCCCCCCCCCCCCDCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCDCCC\n",
      "145373.0                 DCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "145288.0                 CCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCC\n",
      "145173.0                 DCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "144777.0                 CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "144514.0                 DCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCDCDCCCCCDCCCDCCCCCCC\n",
      "144352.0                 DCCCCCCCCCCCCCCCDCCCCCCCCCDCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "144054.0                 DCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "143979.0                 CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCDCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "143951.0                 CCCCDCCCCCCCCDCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "143731.0                 DCCCCCCCCCCCCCDCDCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "143615.0                 DCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "for policy in sorted(mean_cumulative_reward_policies, key=lambda x: mean_cumulative_reward_policies[x], reverse=True):\n",
    "    print(f\"{mean_cumulative_reward_policies[policy]:<24} {policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCCCDCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCDCCCCCCCCDCCCCCCCCCCCCCCCCCC']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_policies([agents[cumulative_rewards[0][0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[258.85244791, 256.07213307],\n",
       "         [263.14667497, 176.88293418],\n",
       "         [267.55473068, 255.29884888],\n",
       "         [249.42754356,  37.11568336]],\n",
       "\n",
       "        [[254.43528773, 269.29796889],\n",
       "         [248.06996277,  61.00552607],\n",
       "         [238.01940145,  11.24905851],\n",
       "         [ 65.01964217,   0.        ]],\n",
       "\n",
       "        [[267.50826318, 265.91731419],\n",
       "         [265.85541276,  77.60073739],\n",
       "         [262.28001163,  92.38964146],\n",
       "         [126.80438918,   0.5       ]],\n",
       "\n",
       "        [[257.17937024,  98.03574845],\n",
       "         [209.38802128,  10.19324086],\n",
       "         [ 89.04572376,   3.49449315],\n",
       "         [ 17.01751472,   0.        ]]],\n",
       "\n",
       "\n",
       "       [[[258.36292192, 211.21590507],\n",
       "         [212.04746573,   0.        ],\n",
       "         [267.92187826, 246.38918269],\n",
       "         [239.53263243,   8.0254341 ]],\n",
       "\n",
       "        [[256.78407515, 172.78224022],\n",
       "         [154.2284775 ,   6.27062235],\n",
       "         [152.45109014,   0.        ],\n",
       "         [  8.17536402,   0.        ]],\n",
       "\n",
       "        [[263.79925288, 112.84056938],\n",
       "         [176.75064028,   3.80691721],\n",
       "         [ 78.43815009,  17.34518115],\n",
       "         [  0.        ,   0.        ]],\n",
       "\n",
       "        [[153.42636744,  29.405476  ],\n",
       "         [  0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ]]],\n",
       "\n",
       "\n",
       "       [[[258.30002215, 268.83918981],\n",
       "         [268.20490518, 202.88431839],\n",
       "         [266.87908084, 148.79041298],\n",
       "         [214.89482139,   0.60899735]],\n",
       "\n",
       "        [[205.12630658, 267.81198223],\n",
       "         [191.8373625 ,   0.7169733 ],\n",
       "         [165.841768  ,  12.15822589],\n",
       "         [ 11.37557647,   0.        ]],\n",
       "\n",
       "        [[267.3199817 , 156.24803935],\n",
       "         [199.18869444,  13.51447719],\n",
       "         [180.89237761,   0.        ],\n",
       "         [ 21.84044709,   0.        ]],\n",
       "\n",
       "        [[212.44301184,   4.37512367],\n",
       "         [  0.        ,   4.63044692],\n",
       "         [ 17.49226166,   0.        ],\n",
       "         [  0.        ,   0.        ]]],\n",
       "\n",
       "\n",
       "       [[[257.99706475, 203.39924849],\n",
       "         [192.81084715,  22.99778628],\n",
       "         [201.09456565,  31.09854222],\n",
       "         [ 18.08541961,   0.        ]],\n",
       "\n",
       "        [[247.04398426,  19.16423073],\n",
       "         [ 86.32114738,   0.        ],\n",
       "         [ 81.35415021,   0.        ],\n",
       "         [  0.        ,   0.        ]],\n",
       "\n",
       "        [[191.51159457,  25.43957731],\n",
       "         [ 63.048763  ,   0.        ],\n",
       "         [ 49.03074376,   0.        ],\n",
       "         [  7.59171613,   0.        ]],\n",
       "\n",
       "        [[ 80.37926698,  13.73041242],\n",
       "         [  0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[cumulative_rewards[0][0]].gameplay_strategy._q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('(C, C)', '(C, C)', '(C, C)', '(C, C)') => C\n",
      "('(C, C)', '(C, C)', '(C, C)', '(C, D)') => C\n",
      "('(C, C)', '(C, C)', '(C, C)', '(D, C)') => C\n",
      "('(C, C)', '(C, C)', '(C, C)', '(D, D)') => C\n",
      "('(C, C)', '(C, C)', '(C, D)', '(C, C)') => D\n",
      "('(C, C)', '(C, C)', '(C, D)', '(C, D)') => C\n",
      "('(C, C)', '(C, C)', '(C, D)', '(D, C)') => C\n",
      "('(C, C)', '(C, C)', '(C, D)', '(D, D)') => C\n",
      "('(C, C)', '(C, C)', '(D, C)', '(C, C)') => C\n",
      "('(C, C)', '(C, C)', '(D, C)', '(C, D)') => C\n",
      "('(C, C)', '(C, C)', '(D, C)', '(D, C)') => C\n",
      "('(C, C)', '(C, C)', '(D, C)', '(D, D)') => C\n",
      "('(C, C)', '(C, C)', '(D, D)', '(C, C)') => C\n",
      "('(C, C)', '(C, C)', '(D, D)', '(C, D)') => C\n",
      "('(C, C)', '(C, C)', '(D, D)', '(D, C)') => C\n",
      "('(C, C)', '(C, C)', '(D, D)', '(D, D)') => C\n",
      "('(C, C)', '(C, D)', '(C, C)', '(C, C)') => C\n",
      "('(C, C)', '(C, D)', '(C, C)', '(C, D)') => C\n",
      "('(C, C)', '(C, D)', '(C, C)', '(D, C)') => C\n",
      "('(C, C)', '(C, D)', '(C, C)', '(D, D)') => C\n",
      "('(C, C)', '(C, D)', '(C, D)', '(C, C)') => C\n",
      "('(C, C)', '(C, D)', '(C, D)', '(C, D)') => C\n",
      "('(C, C)', '(C, D)', '(C, D)', '(D, C)') => C\n",
      "('(C, C)', '(C, D)', '(C, D)', '(D, D)') => C\n",
      "('(C, C)', '(C, D)', '(D, C)', '(C, C)') => C\n",
      "('(C, C)', '(C, D)', '(D, C)', '(C, D)') => C\n",
      "('(C, C)', '(C, D)', '(D, C)', '(D, C)') => C\n",
      "('(C, C)', '(C, D)', '(D, C)', '(D, D)') => C\n",
      "('(C, C)', '(C, D)', '(D, D)', '(C, C)') => C\n",
      "('(C, C)', '(C, D)', '(D, D)', '(C, D)') => C\n",
      "('(C, C)', '(C, D)', '(D, D)', '(D, C)') => C\n",
      "('(C, C)', '(C, D)', '(D, D)', '(D, D)') => C\n",
      "('(C, C)', '(D, C)', '(C, C)', '(C, C)') => D\n",
      "('(C, C)', '(D, C)', '(C, C)', '(C, D)') => C\n",
      "('(C, C)', '(D, C)', '(C, C)', '(D, C)') => C\n",
      "('(C, C)', '(D, C)', '(C, C)', '(D, D)') => C\n",
      "('(C, C)', '(D, C)', '(C, D)', '(C, C)') => D\n",
      "('(C, C)', '(D, C)', '(C, D)', '(C, D)') => C\n",
      "('(C, C)', '(D, C)', '(C, D)', '(D, C)') => C\n",
      "('(C, C)', '(D, C)', '(C, D)', '(D, D)') => C\n",
      "('(C, C)', '(D, C)', '(D, C)', '(C, C)') => C\n",
      "('(C, C)', '(D, C)', '(D, C)', '(C, D)') => C\n",
      "('(C, C)', '(D, C)', '(D, C)', '(D, C)') => C\n",
      "('(C, C)', '(D, C)', '(D, C)', '(D, D)') => C\n",
      "('(C, C)', '(D, C)', '(D, D)', '(C, C)') => C\n",
      "('(C, C)', '(D, C)', '(D, D)', '(C, D)') => D\n",
      "('(C, C)', '(D, C)', '(D, D)', '(D, C)') => C\n",
      "('(C, C)', '(D, C)', '(D, D)', '(D, D)') => C\n",
      "('(C, C)', '(D, D)', '(C, C)', '(C, C)') => C\n",
      "('(C, C)', '(D, D)', '(C, C)', '(C, D)') => C\n",
      "('(C, C)', '(D, D)', '(C, C)', '(D, C)') => C\n",
      "('(C, C)', '(D, D)', '(C, C)', '(D, D)') => C\n",
      "('(C, C)', '(D, D)', '(C, D)', '(C, C)') => C\n",
      "('(C, C)', '(D, D)', '(C, D)', '(C, D)') => C\n",
      "('(C, C)', '(D, D)', '(C, D)', '(D, C)') => C\n",
      "('(C, C)', '(D, D)', '(C, D)', '(D, D)') => C\n",
      "('(C, C)', '(D, D)', '(D, C)', '(C, C)') => C\n",
      "('(C, C)', '(D, D)', '(D, C)', '(C, D)') => C\n",
      "('(C, C)', '(D, D)', '(D, C)', '(D, C)') => C\n",
      "('(C, C)', '(D, D)', '(D, C)', '(D, D)') => C\n",
      "('(C, C)', '(D, D)', '(D, D)', '(C, C)') => C\n",
      "('(C, C)', '(D, D)', '(D, D)', '(C, D)') => C\n",
      "('(C, C)', '(D, D)', '(D, D)', '(D, C)') => C\n",
      "('(C, C)', '(D, D)', '(D, D)', '(D, D)') => C\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "P = compute_policies([agents[cumulative_rewards[0][0]]])[0]\n",
    "\n",
    "for m, p in zip(product(PAYOFF_LABELS, PAYOFF_LABELS, PAYOFF_LABELS, PAYOFF_LABELS), P):\n",
    "    print(m, \"=>\", p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea8fc3f6c7fa3223c55434ded24c0b3b1260e6c6965b48cfc542164a121bf59d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
