{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from random import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from society.action import Action\n",
    "from society.agent import Agent\n",
    "from society.simulations.adaptive import AdaptiveSimulation\n",
    "from society.agents.constant import AllC, AllD\n",
    "from society.agents.qlearning import TabularQLearner\n",
    "from society.agents.random import Random\n",
    "from society.agents.tft import TitForTat\n",
    "from society.visualisation.network import *\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_population(communities, size):\n",
    "    population = communities * size\n",
    "\n",
    "    agents = [TabularQLearner(lookback=3) for i in range(population)]\n",
    "\n",
    "    G = nx.connected_caveman_graph(communities, size)\n",
    "\n",
    "    weights_matrix = np.zeros((population, population))\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        weights_matrix[u, v] = weights_matrix[v, u] = 1.0\n",
    "        try:\n",
    "            d[\"weight\"] = weights_matrix[u, v]\n",
    "        except:\n",
    "            d[\"weight\"] = 0\n",
    "\n",
    "    return agents, weights_matrix, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policies(agents, rankings):\n",
    "    policies = [\n",
    "        (agents[partner[0]]._q_table.argmax(axis=-1), partner[1])\n",
    "        for partner in rankings\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        (\n",
    "            \"\".join(\n",
    "                [\n",
    "                    (\"C\", \"D\")[policy[0][i, j, k]]\n",
    "                    for i in range(4)\n",
    "                    for j in range(4)\n",
    "                    for k in range(4)\n",
    "                ]\n",
    "            ),\n",
    "            policy[1],\n",
    "        )\n",
    "        for policy in policies\n",
    "    ]\n",
    "\n",
    "\n",
    "def calculate_cooperativeness(history):\n",
    "    count = history.count(Action.COOPERATE)\n",
    "\n",
    "    return count / len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_LABELS = [\"(C, C)\", \"(C, D)\", \"(D, C)\", \"(D, D)\"]\n",
    "POPULATION = (2, 4)\n",
    "ROUNDS = 20_000\n",
    "\n",
    "cumulative_reward_results = []\n",
    "cooperativeness_results = []\n",
    "\n",
    "for run in range(20):\n",
    "    # Generate a new population\n",
    "    agents, weights_matrix, G = generate_population(*POPULATION)\n",
    "\n",
    "    # Run a number of rounds\n",
    "    sim = AdaptiveSimulation(agents, weights_matrix)\n",
    "    sim.reset()\n",
    "    for i in tqdm(range(ROUNDS), desc=f\"Run {run + 1}\"):\n",
    "        sim.play_round()\n",
    "\n",
    "    # Rank agents by cumulative reward\n",
    "    cumulative_rewards_matrix = [\n",
    "        [sum(r) if len(r) > 0 else 0 for r in agent] for agent in sim.rewards\n",
    "    ]\n",
    "    cumulative_reward_rankings = sorted(\n",
    "        [(i, sum(cumulative_rewards_matrix[i])) for i in range(len(agents))],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"BEST REWARD: {cumulative_reward_rankings[0][1]:<28} WORST REWARD: {cumulative_reward_rankings[-1][1]}\"\n",
    "    )\n",
    "\n",
    "    cumulative_reward_results.append(\n",
    "        compute_policies(agents, cumulative_reward_rankings)\n",
    "    )\n",
    "\n",
    "    # Rank agents by cooperativeness\n",
    "    cooperativeness_rankings = sorted(\n",
    "        [\n",
    "            (i, calculate_cooperativeness(list(chain(*history))))\n",
    "            for i, history in enumerate(sim.action_histories)\n",
    "        ],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"BEST COOPERATOR: {cooperativeness_rankings[0][1]:<24} WORST COOPERATOR: {cooperativeness_rankings[-1][1]}\"\n",
    "    )\n",
    "\n",
    "    cooperativeness_results.append(compute_policies(agents, cooperativeness_rankings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_reward_policy_ranks = {}\n",
    "\n",
    "for result in cumulative_reward_results:\n",
    "    for rank, (policy, score) in enumerate(result):\n",
    "        if policy not in cumulative_reward_policy_ranks:\n",
    "            cumulative_reward_policy_ranks[policy] = []\n",
    "\n",
    "        cumulative_reward_policy_ranks[policy].append(rank + 1)\n",
    "\n",
    "mean_cumulative_reward_policy_ranks = {\n",
    "    policy: np.mean(ranks) for policy, ranks in cumulative_reward_policy_ranks.items()\n",
    "}\n",
    "\n",
    "for policy in sorted(\n",
    "    mean_cumulative_reward_policy_ranks,\n",
    "    key=lambda x: mean_cumulative_reward_policy_ranks[x],\n",
    "):\n",
    "    print(f\"{mean_cumulative_reward_policy_ranks[policy]:<24} {policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooperativeness_policy_ranks = {}\n",
    "cooperativeness_policy_scores = {}\n",
    "\n",
    "for result in cooperativeness_results:\n",
    "    for rank, (policy, score) in enumerate(result):\n",
    "        if policy not in cooperativeness_policy_ranks:\n",
    "            cooperativeness_policy_ranks[policy] = []\n",
    "            cooperativeness_policy_scores[policy] = []\n",
    "\n",
    "        cooperativeness_policy_ranks[policy].append(rank + 1)\n",
    "        cooperativeness_policy_scores[policy].append(score)\n",
    "\n",
    "mean_cooperativeness_policy_ranks = {\n",
    "    policy: (np.mean(ranks), len(ranks))\n",
    "    for policy, ranks in cooperativeness_policy_ranks.items()\n",
    "}\n",
    "\n",
    "mean_cooperativeness_policy_scores = {\n",
    "    policy: (np.mean(scores), len(scores))\n",
    "    for policy, scores in cooperativeness_policy_scores.items()\n",
    "}\n",
    "\n",
    "for policy in sorted(\n",
    "    mean_cooperativeness_policy_ranks,\n",
    "    key=lambda x: mean_cooperativeness_policy_ranks[x][0],\n",
    "):\n",
    "    print(\n",
    "        f\"{mean_cooperativeness_policy_ranks[policy][0]:<24} {policy:<24} {mean_cooperativeness_policy_ranks[policy][1]:<8} {mean_cooperativeness_policy_scores[policy][0]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for policy in sorted(\n",
    "    mean_cooperativeness_policy_scores,\n",
    "    key=lambda x: mean_cooperativeness_policy_scores[x][0],\n",
    "    reverse=True,\n",
    "):\n",
    "    print(\n",
    "        f\"{mean_cooperativeness_policy_ranks[policy][0]:<24} {policy:<24} {mean_cooperativeness_policy_ranks[policy][1]:<8} {mean_cooperativeness_policy_scores[policy][0]}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea8fc3f6c7fa3223c55434ded24c0b3b1260e6c6965b48cfc542164a121bf59d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
